import requests
import time
import os
from datetime import datetime
from urllib.parse import quote
from config import *
from database import AdDatabase
from ad_extractor import GoogleAdExtractor


class GoogleSERPScraper:
    """Google SERPæ•°æ®æŠ“å–å™¨"""
    
    def __init__(self):
        self.api_key = API_KEY
        self.zone = ZONE
        self.db = AdDatabase()
        self.ad_extractor = GoogleAdExtractor()
        self.session = requests.Session()
        
        # ç¡®ä¿HTMLç›®å½•å­˜åœ¨
        os.makedirs(HTML_DIR, exist_ok=True)
    
    def scrape_keyword_country(self, keyword, country_code):
        """æŠ“å–ç‰¹å®šå…³é”®è¯åœ¨ç‰¹å®šå›½å®¶çš„SERPæ•°æ®"""
        print(f"å¼€å§‹æŠ“å–å…³é”®è¯ '{keyword}' åœ¨ {country_code} çš„æ•°æ®...")
        
        scrape_time = datetime.now()
        
        try:
            # è·å–SERPæ•°æ®
            html_content = self._fetch_serp_data(keyword, country_code)
            
            if not html_content:
                self.db.insert_scrape_log(
                    keyword, country_code, "failed", 
                    error_message="æ— æ³•è·å–SERPæ•°æ®"
                )
                return False
            
            # ä¿å­˜HTMLæ–‡ä»¶
            html_file_path = self._save_html_file(
                html_content, keyword, country_code, scrape_time
            )
            
            # æå–å¹¿å‘Šæ•°æ®
            ads_data = self.ad_extractor.extract_ads(html_content)
            
            # ä¿å­˜å¹¿å‘Šæ•°æ®åˆ°æ•°æ®åº“
            ads_saved = 0
            for ad_data in ads_data:
                if self.db.insert_ad_data(
                    keyword, country_code, ad_data, scrape_time, html_file_path
                ):
                    ads_saved += 1
            
            # è®°å½•æŠ“å–æ—¥å¿—
            self.db.insert_scrape_log(
                keyword, country_code, "success", ads_found=ads_saved
            )
            
            print(f"âœ… æˆåŠŸæŠ“å– {ads_saved} ä¸ªå¹¿å‘Š")
            return True
            
        except Exception as e:
            error_msg = f"æŠ“å–å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            self.db.insert_scrape_log(
                keyword, country_code, "failed", error_message=error_msg
            )
            return False
    
    def _fetch_serp_data(self, keyword, country_code):
        """ä½¿ç”¨Bright Data APIè·å–SERPæ•°æ®"""
        url = "https://api.brightdata.com/request"
        
        search_url = f"https://www.google.com/search?q={quote(keyword)}"
        
        payload = {
            "zone": self.zone,
            "url": search_url,
            "format": "json",
            "method": "GET",
            "country": country_code,
            "data_format": "html"
        }
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(MAX_RETRIES):
            try:
                print(f"  å‘é€APIè¯·æ±‚ (å°è¯• {attempt + 1}/{MAX_RETRIES})...")
                
                response = self.session.post(
                    url, 
                    json=payload, 
                    headers=headers, 
                    timeout=REQUEST_TIMEOUT
                )
                
                if response.status_code == 200:
                    result = response.json()
                    
                    # æ ¹æ®å®é™…APIå“åº”ç»“æ„æå–HTMLå†…å®¹
                    html_content = self._extract_html_from_response(result)
                    
                    if html_content:
                        print(f"  âœ… APIè¯·æ±‚æˆåŠŸï¼Œè·å¾—HTMLå†…å®¹ ({len(html_content)} å­—ç¬¦)")
                        return html_content
                    else:
                        print(f"  âš ï¸ APIå“åº”ä¸­æœªæ‰¾åˆ°HTMLå†…å®¹")
                        
                else:
                    print(f"  âŒ APIè¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
                    print(f"  å“åº”å†…å®¹: {response.text[:500]}")
                
            except requests.exceptions.RequestException as e:
                print(f"  âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}")
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
            if attempt < MAX_RETRIES - 1:
                print(f"  ç­‰å¾… {RETRY_DELAY} ç§’åé‡è¯•...")
                time.sleep(RETRY_DELAY)
        
        return None
    
    def _extract_html_from_response(self, api_response):
        """ä»APIå“åº”ä¸­æå–HTMLå†…å®¹"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„Bright Data APIå“åº”æ ¼å¼è¿›è¡Œè°ƒæ•´
        # å¸¸è§çš„å“åº”æ ¼å¼å¯èƒ½åŒ…æ‹¬ï¼š
        
        if isinstance(api_response, dict):
            # å°è¯•ä¸åŒçš„å¯èƒ½çš„HTMLå­—æ®µå
            html_fields = ['html', 'body', 'content', 'data', 'page_content']
            
            for field in html_fields:
                if field in api_response:
                    content = api_response[field]
                    if isinstance(content, str) and len(content) > 100:
                        return content
            
            # å¦‚æœæ˜¯åµŒå¥—ç»“æ„
            if 'data' in api_response and isinstance(api_response['data'], dict):
                for field in html_fields:
                    if field in api_response['data']:
                        content = api_response['data'][field]
                        if isinstance(content, str) and len(content) > 100:
                            return content
        
        elif isinstance(api_response, str):
            # å¦‚æœå“åº”ç›´æ¥å°±æ˜¯HTMLå­—ç¬¦ä¸²
            if len(api_response) > 100:
                return api_response
        
        return None
    
    def _save_html_file(self, html_content, keyword, country_code, scrape_time):
        """ä¿å­˜HTMLæ–‡ä»¶"""
        # åˆ›å»ºæ–‡ä»¶åï¼Œä½¿ç”¨æ—¶é—´æˆ³é¿å…å†²çª
        timestamp = scrape_time.strftime("%Y%m%d_%H%M%S")
        safe_keyword = "".join(c for c in keyword if c.isalnum() or c in (' ', '-', '_')).strip()
        safe_keyword = safe_keyword.replace(' ', '_')
        
        filename = f"{timestamp}_{country_code}_{safe_keyword}.html"
        file_path = os.path.join(HTML_DIR, filename)
        
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            print(f"  ğŸ’¾ HTMLæ–‡ä»¶å·²ä¿å­˜: {filename}")
            return file_path
        except Exception as e:
            print(f"  âš ï¸ ä¿å­˜HTMLæ–‡ä»¶å¤±è´¥: {str(e)}")
            return None
    
    def scrape_all_combinations(self):
        """æŠ“å–æ‰€æœ‰å…³é”®è¯å’Œå›½å®¶çš„ç»„åˆ"""
        total_combinations = len(KEYWORDS_LIST) * len(COUNTRY_LIST)
        current = 0
        successful = 0
        
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡æŠ“å–ï¼Œå…± {total_combinations} ä¸ªç»„åˆ")
        print(f"å…³é”®è¯æ•°é‡: {len(KEYWORDS_LIST)}")
        print(f"å›½å®¶æ•°é‡: {len(COUNTRY_LIST)}")
        print("-" * 60)
        
        start_time = datetime.now()
        
        for keyword in KEYWORDS_LIST:
            for country_code in COUNTRY_LIST:
                current += 1
                
                print(f"\n[{current}/{total_combinations}] ", end="")
                
                success = self.scrape_keyword_country(keyword, country_code)
                if success:
                    successful += 1
                
                # åœ¨è¯·æ±‚ä¹‹é—´æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¢«é™åˆ¶
                if current < total_combinations:
                    print(f"  â³ ç­‰å¾… {RETRY_DELAY} ç§’...")
                    time.sleep(RETRY_DELAY)
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        print("\n" + "=" * 60)
        print(f"ğŸ‰ æ‰¹é‡æŠ“å–å®Œæˆ!")
        print(f"æ€»ç”¨æ—¶: {duration}")
        print(f"æˆåŠŸç‡: {successful}/{total_combinations} ({successful/total_combinations*100:.1f}%)")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        self.show_stats()
    
    def scrape_single(self, keyword, country_code):
        """æŠ“å–å•ä¸ªå…³é”®è¯å’Œå›½å®¶ç»„åˆ"""
        print(f"ğŸ¯ å•æ¬¡æŠ“å–æ¨¡å¼")
        print(f"å…³é”®è¯: {keyword}")
        print(f"å›½å®¶: {country_code}")
        print("-" * 40)
        
        success = self.scrape_keyword_country(keyword, country_code)
        
        if success:
            print(f"âœ… æŠ“å–å®Œæˆ")
        else:
            print(f"âŒ æŠ“å–å¤±è´¥")
        
        self.show_stats()
    
    def show_stats(self):
        """æ˜¾ç¤ºæŠ“å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.db.get_scrape_stats()
        
        print("\nğŸ“Š æŠ“å–ç»Ÿè®¡:")
        print(f"  æ€»æŠ“å–æ¬¡æ•°: {stats['total_scrapes']}")
        print(f"  æˆåŠŸæ¬¡æ•°: {stats['successful_scrapes']}")
        print(f"  å‘ç°çš„å¹¿å‘Šæ€»æ•°: {stats['total_ads']}")
        print(f"  æœ€åæŠ“å–æ—¶é—´: {stats['last_scrape'] or 'æ— '}")


if __name__ == "__main__":
    # æ£€æŸ¥é…ç½®
    if API_KEY == "your_api_key_here" or ZONE == "your_zone_here":
        print("âŒ è¯·å…ˆåœ¨ config.py ä¸­é…ç½® API_KEY å’Œ ZONE")
        exit(1)
    
    scraper = GoogleSERPScraper()
    
    # å¯ä»¥é€‰æ‹©è¿è¡Œæ¨¡å¼
    import sys
    
    if len(sys.argv) == 3:
        # å•ä¸ªå…³é”®è¯æ¨¡å¼: python scraper.py "keyword" "country"
        keyword = sys.argv[1]
        country = sys.argv[2]
        scraper.scrape_single(keyword, country)
    else:
        # æ‰¹é‡æ¨¡å¼
        scraper.scrape_all_combinations()