#!/usr/bin/env python3
"""
å¹¿å‘Šè§£ææµ‹è¯•ä»£ç 
ä½¿ç”¨æ–°çš„è§£ææ–¹æ³•æµ‹è¯•ç°æœ‰HTMLæ–‡ä»¶ä¸­çš„å¹¿å‘Šæå–æ•ˆæœ
"""

import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import parse_qs, urlparse, unquote
import re


class TestAdParser:
    """æµ‹è¯•å¹¿å‘Šè§£æå™¨"""
    
    def __init__(self):
        self.session = requests.Session()
        # è®¾ç½®å®Œæ•´çš„è¯·æ±‚å¤´æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"macOS"'
        })
    
    def parse_html_file(self, file_path):
        """è§£æå•ä¸ªHTMLæ–‡ä»¶"""
        print(f"\nğŸ” è§£ææ–‡ä»¶: {os.path.basename(file_path)}")
        print("=" * 60)
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            return []
        
        soup = BeautifulSoup(html_content, 'html.parser')
        ads = self.extract_ads_with_data_rw(soup)
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(ads)} ä¸ªå¹¿å‘Š")
        
        for i, ad in enumerate(ads, 1):
            print(f"\nğŸ“¢ å¹¿å‘Š #{i}")
            print(f"   æ ‡é¢˜: {ad.get('title', 'æ— ')}")
            print(f"   å¹¿å‘ŠURL: {ad.get('ad_url', 'æ— ')[:100]}...")
            print(f"   ç›®æ ‡URL: {ad.get('target_url', 'æ— ')}")
            print(f"   æè¿°: {ad.get('description', 'æ— ')}")
            
            # è®¿é—®å¹¿å‘ŠURLè·å–çœŸå®çš„ç›®æ ‡åœ°å€
            if ad.get('ad_url'):
                print(f"   ğŸ”„ æ­£åœ¨è®¿é—®å¹¿å‘ŠURLè·å–çœŸå®ç›®æ ‡åœ°å€...")
                real_target_result = self.get_real_target_url(ad['ad_url'])
                if real_target_result:
                    print(f"   âœ… çœŸå®ç›®æ ‡URL: {real_target_result['final_url']}")
                    if real_target_result['ref_parameter']:
                        print(f"   ğŸ¯ Refå‚æ•°: {real_target_result['ref_parameter']}")
                        ad['ref_parameter'] = real_target_result['ref_parameter']
                    ad['real_target_url'] = real_target_result['final_url']
                else:
                    print(f"   âŒ æ— æ³•è·å–çœŸå®ç›®æ ‡URL")
        
        return ads
    
    def extract_ads_with_data_rw(self, soup):
        """ä½¿ç”¨data-rwå±æ€§æå–å¹¿å‘Š"""
        ads = []
        
        # æŸ¥æ‰¾æ‰€æœ‰å¸¦æœ‰data-rwå±æ€§çš„aæ ‡ç­¾
        ad_links = soup.find_all('a', {'data-rw': True})
        
        print(f"ğŸ¯ æ‰¾åˆ° {len(ad_links)} ä¸ªå¸¦æœ‰data-rwå±æ€§çš„é“¾æ¥")
        
        for i, link in enumerate(ad_links, 1):
            print(f"\nåˆ†æé“¾æ¥ #{i}:")
            
            # è·å–å¹¿å‘ŠURL (data-rwå±æ€§å€¼)
            ad_url = link.get('data-rw', '')
            print(f"  data-rw: {ad_url[:80]}...")
            
            # è§£æç›®æ ‡URL
            target_url = self.extract_target_url_from_ad_url(ad_url)
            print(f"  è§£æçš„ç›®æ ‡URL: {target_url}")
            
            # æŸ¥æ‰¾å¹¿å‘Šæ ‡é¢˜
            title = self.extract_title_from_ad_element(link)
            print(f"  æ ‡é¢˜: {title}")
            
            # æŸ¥æ‰¾å¹¿å‘Šæè¿°
            description = self.extract_description_from_ad_element(link)
            print(f"  æè¿°: {description}")
            
            # éªŒè¯è¿™æ˜¯å¦æ˜¯æœ‰æ•ˆå¹¿å‘Š
            if self.is_valid_ad_data(title, ad_url, target_url):
                ads.append({
                    'title': title,
                    'ad_url': ad_url,
                    'target_url': target_url,
                    'description': description,
                    'position': i
                })
                print(f"  âœ… æœ‰æ•ˆå¹¿å‘Š")
            else:
                print(f"  âŒ æ— æ•ˆå¹¿å‘Šï¼Œè·³è¿‡")
        
        return ads
    
    def extract_target_url_from_ad_url(self, ad_url):
        """ä»Googleå¹¿å‘ŠURLä¸­æå–çœŸå®çš„ç›®æ ‡URL"""
        if not ad_url:
            return "æ— "
        
        try:
            # Googleå¹¿å‘ŠURLé€šå¸¸åŒ…å«å¤šä¸ªå‚æ•°ï¼ŒçœŸå®URLå¯èƒ½åœ¨ä¸åŒå‚æ•°ä¸­
            parsed_url = urlparse(ad_url)
            query_params = parse_qs(parsed_url.query)
            
            # å¸¸è§çš„ç›®æ ‡URLå‚æ•°å
            url_param_names = ['url', 'adurl', 'q', 'dest_url', 'continue']
            
            for param_name in url_param_names:
                if param_name in query_params:
                    target_url = query_params[param_name][0]
                    if target_url.startswith('http'):
                        return unquote(target_url)
            
            # å¦‚æœåœ¨æŸ¥è¯¢å‚æ•°ä¸­æ²¡æ‰¾åˆ°ï¼Œå°è¯•åœ¨URLè·¯å¾„ä¸­æŸ¥æ‰¾
            # æœ‰æ—¶ç›®æ ‡URLç›´æ¥ç¼–ç åœ¨è·¯å¾„ä¸­
            url_patterns = [
                r'url=([^&]+)',
                r'adurl=([^&]+)',
                r'dest_url=([^&]+)'
            ]
            
            for pattern in url_patterns:
                match = re.search(pattern, ad_url)
                if match:
                    target_url = unquote(match.group(1))
                    if target_url.startswith('http'):
                        return target_url
            
            return "æ— æ³•è§£æ"
            
        except Exception as e:
            print(f"    è§£æURLæ—¶å‡ºé”™: {e}")
            return "è§£æå‡ºé”™"
    
    def extract_title_from_ad_element(self, link_element):
        """ä»å¹¿å‘Šå…ƒç´ ä¸­æå–æ ‡é¢˜"""
        # å…ˆåœ¨é“¾æ¥æœ¬èº«ä¸­æŸ¥æ‰¾æ ‡é¢˜
        title_selectors = [
            # ç›´æ¥å­å…ƒç´ ä¸­çš„æ ‡é¢˜
            'div[role="heading"]',
            'h1', 'h2', 'h3', 'h4',
            # å¸¸è§çš„Googleå¹¿å‘Šæ ‡é¢˜ç±»
            '.CCgQ5', '.vCa9Yd', '.QfkTvb', '.MBeuO', '.Va3FIb',
            # å…¶ä»–å¯èƒ½çš„æ ‡é¢˜å®¹å™¨
            '[aria-level="3"]',
            'span'
        ]
        
        for selector in title_selectors:
            title_elem = link_element.select_one(selector)
            if title_elem:
                title_text = title_elem.get_text().strip()
                if title_text and len(title_text) > 3:
                    return title_text
        
        # å¦‚æœåœ¨é“¾æ¥å†…æ²¡æ‰¾åˆ°ï¼Œå‘ä¸ŠæŸ¥æ‰¾çˆ¶å®¹å™¨
        parent = link_element.parent
        for _ in range(3):  # æœ€å¤šå‘ä¸ŠæŸ¥æ‰¾3å±‚
            if not parent:
                break
            
            for selector in title_selectors:
                title_elem = parent.select_one(selector)
                if title_elem:
                    title_text = title_elem.get_text().strip()
                    if title_text and len(title_text) > 3:
                        return title_text
            
            parent = parent.parent
        
        # æœ€åå°è¯•ä½¿ç”¨é“¾æ¥çš„ç›´æ¥æ–‡æœ¬å†…å®¹
        link_text = link_element.get_text().strip()
        if link_text and len(link_text) > 3:
            return link_text
        
        return "æ— æ ‡é¢˜"
    
    def extract_description_from_ad_element(self, link_element):
        """ä»å¹¿å‘Šå…ƒç´ ä¸­æå–æè¿°"""
        # æè¿°é€šå¸¸åœ¨å¹¿å‘Šå®¹å™¨ä¸­ï¼Œä½†ä¸æ˜¯æ ‡é¢˜
        desc_selectors = [
            '.VwiC3b', '.yXK7lf', '.s3v9rd', '.st', '.IsZvec',
            '.BNeawe', '.UPmit'
        ]
        
        # åœ¨å½“å‰å…ƒç´ ä¸­æŸ¥æ‰¾
        for selector in desc_selectors:
            desc_elem = link_element.select_one(selector)
            if desc_elem:
                desc_text = desc_elem.get_text().strip()
                if desc_text and len(desc_text) > 10:
                    return desc_text
        
        # åœ¨çˆ¶å®¹å™¨ä¸­æŸ¥æ‰¾
        parent = link_element.parent
        for _ in range(3):
            if not parent:
                break
            
            for selector in desc_selectors:
                desc_elem = parent.select_one(selector)
                if desc_elem:
                    desc_text = desc_elem.get_text().strip()
                    if desc_text and len(desc_text) > 10:
                        return desc_text
            
            parent = parent.parent
        
        return "æ— æè¿°"
    
    def is_valid_ad_data(self, title, ad_url, target_url):
        """éªŒè¯å¹¿å‘Šæ•°æ®æ˜¯å¦æœ‰æ•ˆ"""
        # å¿…é¡»æœ‰æ ‡é¢˜æˆ–æœ‰æ•ˆçš„URL
        if not title or title == "æ— æ ‡é¢˜":
            if not ad_url:
                return False
        
        # æ ‡é¢˜ä¸èƒ½å¤ªçŸ­
        if title and title != "æ— æ ‡é¢˜" and len(title.strip()) < 3:
            return False
        
        # å¿…é¡»æœ‰å¹¿å‘ŠURL
        if not ad_url:
            return False
        
        return True
    
    def get_real_target_url(self, ad_url):
        """è®¿é—®å¹¿å‘ŠURLè·å–çœŸå®çš„ç›®æ ‡åœ°å€"""
        try:
            print(f"      è®¿é—®: {ad_url[:80]}...")
            
            # ä½¿ç”¨GETè¯·æ±‚è®¿é—®å¹¿å‘ŠURLï¼Œå…è®¸é‡å®šå‘
            # æ·»åŠ Refererå¤´æ¥æ¨¡æ‹Ÿä»Googleæœç´¢é¡µé¢ç‚¹å‡»
            headers = {
                'Referer': 'https://www.google.com/',
                'Cache-Control': 'max-age=0'
            }
            
            response = self.session.get(
                ad_url,
                allow_redirects=True,
                timeout=15,
                headers=headers
            )
            
            final_url = response.url
            print(f"      çŠ¶æ€ç : {response.status_code}")
            print(f"      æœ€ç»ˆURL: {final_url[:80]}...")
            
            # æå–URLä¸­çš„refå‚æ•°
            ref_value = self.extract_ref_parameter(final_url)
            if ref_value:
                print(f"      ğŸ¯ æå–åˆ°refå‚æ•°: {ref_value}")
            else:
                print(f"      âš ï¸ æœªæ‰¾åˆ°refå‚æ•°")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ç›®æ ‡ç½‘ç«™
            if self.is_valid_target_url(final_url):
                result = {
                    'final_url': final_url,
                    'ref_parameter': ref_value
                }
                return result
            else:
                print(f"      âš ï¸ å¯èƒ½ä¸æ˜¯æœ‰æ•ˆçš„ç›®æ ‡URL")
                result = {
                    'final_url': final_url,
                    'ref_parameter': ref_value
                }
                return result  # ä»ç„¶è¿”å›ï¼Œè®©ç”¨æˆ·åˆ¤æ–­
                
        except requests.exceptions.Timeout:
            print(f"      â° è¯·æ±‚è¶…æ—¶")
            return None
        except requests.exceptions.RequestException as e:
            print(f"      âŒ è¯·æ±‚å¤±è´¥: {e}")
            return None
        except Exception as e:
            print(f"      âŒ è·å–çœŸå®URLæ—¶å‡ºé”™: {e}")
            return None
    
    def extract_ref_parameter(self, url):
        """ä»URLä¸­æå–refå‚æ•°"""
        try:
            parsed_url = urlparse(url)
            query_params = parse_qs(parsed_url.query)
            
            # æŸ¥æ‰¾refå‚æ•°ï¼Œå¯èƒ½æœ‰ä¸åŒçš„å½¢å¼
            ref_param_names = ['ref', 'referrer', 'reference', 'utm_source', 'source']
            
            for param_name in ref_param_names:
                if param_name in query_params:
                    ref_value = query_params[param_name][0]
                    if ref_value:
                        return ref_value
            
            return None
            
        except Exception as e:
            print(f"      æå–refå‚æ•°æ—¶å‡ºé”™: {e}")
            return None
    
    def is_valid_target_url(self, url):
        """æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ç›®æ ‡URL"""
        if not url:
            return False
        
        # æ’é™¤æ˜æ˜¾ä¸æ˜¯ç›®æ ‡ç½‘ç«™çš„URL
        invalid_domains = [
            'google.com', 'googleadservices.com', 'googlesyndication.com',
            'doubleclick.net', 'googletagmanager.com'
        ]
        
        parsed_url = urlparse(url)
        domain = parsed_url.netloc.lower()
        
        for invalid_domain in invalid_domains:
            if invalid_domain in domain:
                return False
        
        return True


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    parser = TestAdParser()
    
    # æŸ¥æ‰¾htmlsç›®å½•ä¸­çš„æ‰€æœ‰HTMLæ–‡ä»¶
    html_dir = "htmls"
    if not os.path.exists(html_dir):
        print(f"âŒ {html_dir} ç›®å½•ä¸å­˜åœ¨")
        return
    
    html_files = [f for f in os.listdir(html_dir) if f.endswith('.html')]
    
    if not html_files:
        print(f"âŒ {html_dir} ç›®å½•ä¸­æ²¡æœ‰HTMLæ–‡ä»¶")
        return
    
    print(f"ğŸš€ å¼€å§‹æµ‹è¯•å¹¿å‘Šè§£æ")
    print(f"å‘ç° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
    
    total_ads = 0
    
    for html_file in html_files:
        file_path = os.path.join(html_dir, html_file)
        ads = parser.parse_html_file(file_path)
        total_ads += len(ads)
    
    print(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print(f"æ€»è®¡å‘ç° {total_ads} ä¸ªå¹¿å‘Š")
    
    if total_ads > 0:
        print(f"\nâœ… è§£ææ–¹æ³•æœ‰æ•ˆï¼Œå»ºè®®ç»§ç»­ä¼˜åŒ–ä¸»ä»£ç ")
    else:
        print(f"\nâš ï¸ æœªå‘ç°å¹¿å‘Šï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è§£æç­–ç•¥")


if __name__ == "__main__":
    main()