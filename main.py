#!/usr/bin/env python3
"""
Google SERPå¹¿å‘ŠæŠ“å–å·¥å…·
ä½¿ç”¨Bright Data APIè·å–Googleæœç´¢ç»“æœé¡µé¢ï¼Œå¹¶æå–å¹¿å‘Šä¿¡æ¯
"""

import sys
import argparse
import traceback
from datetime import datetime
from scraper import GoogleSERPScraper
from database import AdDatabase
from email_sender import EmailSender
from config import KEYWORDS_LIST, COUNTRY_LIST, EMAIL_CONFIG
from logger import project_logger


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    parser = argparse.ArgumentParser(description='Google SERPå¹¿å‘Šæ•°æ®æŠ“å–å·¥å…·')
    
    parser.add_argument(
        '--mode', 
        choices=['single', 'batch', 'stats'], 
        default='batch',
        help='è¿è¡Œæ¨¡å¼: single(å•æ¬¡æŠ“å–), batch(æ‰¹é‡æŠ“å–), stats(æ˜¾ç¤ºç»Ÿè®¡)'
    )
    
    parser.add_argument(
        '--keyword', 
        type=str, 
        help='è¦æŠ“å–çš„å…³é”®è¯ (singleæ¨¡å¼å¿…éœ€)'
    )
    
    parser.add_argument(
        '--country', 
        type=str, 
        help='ç›®æ ‡å›½å®¶ä»£ç  (singleæ¨¡å¼å¿…éœ€)'
    )
    
    parser.add_argument(
        '--list-config', 
        action='store_true',
        help='æ˜¾ç¤ºå½“å‰é…ç½®çš„å…³é”®è¯å’Œå›½å®¶åˆ—è¡¨'
    )
    
    parser.add_argument(
        '--export', 
        type=str,
        help='å¯¼å‡ºæ•°æ®åˆ°CSVæ–‡ä»¶ (æä¾›æ–‡ä»¶å)'
    )
    
    args = parser.parse_args()
    
    # æ˜¾ç¤ºé…ç½®
    if args.list_config:
        show_config()
        return
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    if args.mode == 'stats':
        show_stats()
        return
    
    # å¯¼å‡ºæ•°æ®
    if args.export:
        export_data(args.export)
        return
    
    # åˆ›å»ºæŠ“å–å™¨
    scraper = GoogleSERPScraper()
    
    # æ ¹æ®æ¨¡å¼è¿è¡Œ
    if args.mode == 'single':
        if not args.keyword or not args.country:
            print("âŒ singleæ¨¡å¼éœ€è¦æä¾› --keyword å’Œ --country å‚æ•°")
            print("ç¤ºä¾‹: python main.py --mode single --keyword 'bingx' --country 'us'")
            return
        
        scraper.scrape_single(args.keyword, args.country)
        
        # å•æ¬¡æŠ“å–å®Œæˆåä¹Ÿå‘é€é‚®ä»¶
       # send_scrape_result_email(scraper)
        
    elif args.mode == 'batch':
        print("ğŸš€ æ‰¹é‡æŠ“å–æ¨¡å¼")
        print(f"å°†æŠ“å– {len(KEYWORDS_LIST)} ä¸ªå…³é”®è¯ Ã— {len(COUNTRY_LIST)} ä¸ªå›½å®¶ = {len(KEYWORDS_LIST) * len(COUNTRY_LIST)} ä¸ªç»„åˆ")
        
        # è¯¢é—®ç”¨æˆ·ç¡®è®¤
      #  confirm = input("æ˜¯å¦ç»§ç»­? (y/n): ").lower().strip()
      #  if confirm != 'y':
      #      print("å·²å–æ¶ˆ")
      #ÃŸ      return
        
        scraper.scrape_all_combinations()
        
        # æŠ“å–å®Œæˆåè‡ªåŠ¨å‘é€é‚®ä»¶
       # send_scrape_result_email(scraper)


def show_config():
    """æ˜¾ç¤ºå½“å‰é…ç½®"""
    print("ğŸ“‹ å½“å‰é…ç½®:")
    print(f"å…³é”®è¯åˆ—è¡¨ ({len(KEYWORDS_LIST)} ä¸ª):")
    for i, keyword in enumerate(KEYWORDS_LIST, 1):
        print(f"  {i}. {keyword}")
    
    print(f"\nå›½å®¶ä»£ç åˆ—è¡¨ ({len(COUNTRY_LIST)} ä¸ª):")
    for i, country in enumerate(COUNTRY_LIST, 1):
        print(f"  {i}. {country}")
    
    print(f"\næ€»æŠ“å–ç»„åˆæ•°: {len(KEYWORDS_LIST) * len(COUNTRY_LIST)}")


def show_stats():
    """æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯"""
    db = AdDatabase()
    stats = db.get_scrape_stats()
    
    print("ğŸ“Š æ•°æ®åº“ç»Ÿè®¡:")
    print(f"  æ€»æŠ“å–æ¬¡æ•°: {stats['total_scrapes']}")
    print(f"  æˆåŠŸæ¬¡æ•°: {stats['successful_scrapes']}")
    print(f"  æˆåŠŸç‡: {stats['successful_scrapes'] / max(stats['total_scrapes'], 1) * 100:.1f}%")
    print(f"  å‘ç°çš„å¹¿å‘Šæ€»æ•°: {stats['total_ads']}")
    print(f"  æœ€åæŠ“å–æ—¶é—´: {stats['last_scrape'] or 'æ— '}")
    
    # æ˜¾ç¤ºæŒ‰å…³é”®è¯åˆ†ç»„çš„ç»Ÿè®¡
    print("\nğŸ“ˆ æŒ‰å…³é”®è¯ç»Ÿè®¡:")
    show_keyword_stats(db)


def show_keyword_stats(db):
    """æ˜¾ç¤ºæŒ‰å…³é”®è¯åˆ†ç»„çš„ç»Ÿè®¡"""
    import sqlite3
    
    conn = sqlite3.connect(db.db_path)
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT keyword, COUNT(*) as ads_count, 
               COUNT(DISTINCT country_code) as countries_count
        FROM ads_data 
        GROUP BY keyword
        ORDER BY ads_count DESC
    ''')
    
    results = cursor.fetchall()
    
    if results:
        for keyword, ads_count, countries_count in results:
            print(f"  {keyword}: {ads_count} ä¸ªå¹¿å‘Š (è¦†ç›– {countries_count} ä¸ªå›½å®¶)")
    else:
        print("  æš‚æ— æ•°æ®")
    
    conn.close()


def export_data(filename):
    """å¯¼å‡ºæ•°æ®åˆ°CSVæ–‡ä»¶"""
    import csv
    import sqlite3
    from database import AdDatabase
    
    db = AdDatabase()
    conn = sqlite3.connect(db.db_path)
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT keyword, country_code, ad_title, ad_url, target_url, 
               real_target_url, ref_parameter, ch_parameter, utm_campaign_parameter,
               ad_description, position, scrape_time
        FROM ads_data
        ORDER BY scrape_time DESC, keyword, country_code, position
    ''')
    
    results = cursor.fetchall()
    
    if not results:
        print("âŒ æ•°æ®åº“ä¸­æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
        return
    
    # ç¡®ä¿æ–‡ä»¶åæœ‰.csvæ‰©å±•å
    if not filename.endswith('.csv'):
        filename += '.csv'
    
    try:
        with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.writer(csvfile)
            
            # å†™å…¥è¡¨å¤´
            writer.writerow([
                'å…³é”®è¯', 'å›½å®¶ä»£ç ', 'å¹¿å‘Šæ ‡é¢˜', 'å¹¿å‘ŠURL', 
                'ç›®æ ‡URL', 'çœŸå®ç›®æ ‡URL', 'Refå‚æ•°', 'Chå‚æ•°', 'UTM Campaignå‚æ•°',
                'å¹¿å‘Šæè¿°', 'ä½ç½®', 'æŠ“å–æ—¶é—´'
            ])
            
            # å†™å…¥æ•°æ®
            writer.writerows(results)
        
        print(f"âœ… æ•°æ®å·²å¯¼å‡ºåˆ° {filename}")
        print(f"   å…±å¯¼å‡º {len(results)} æ¡è®°å½•")
        
    except Exception as e:
        print(f"âŒ å¯¼å‡ºå¤±è´¥: {str(e)}")
    
    conn.close()


def send_scrape_result_email(scraper):
    """å‘é€æŠ“å–ç»“æœé‚®ä»¶åˆ°272363364@qq.com"""
    logger = project_logger.get_logger('main_email', 'main_email.log')
    
    try:
        logger.info("ğŸš€ å¼€å§‹å‘é€æŠ“å–ç»“æœé‚®ä»¶")
        
        # åˆ›å»ºé‚®ä»¶å‘é€å™¨
        email_sender = EmailSender()
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = scraper.db.get_scrape_stats()
        
        # æ„å»ºé‚®ä»¶ä¸»é¢˜
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        subject = f"[æŠ“å–å®Œæˆ] Googleå¹¿å‘Šæ•°æ®æŠ“å–ç»“æœ - {current_time}"
        
        # å‘é€é‚®ä»¶ç»™272363364@qq.com
        recipients = ["272363364@qq.com"]
        
        success = email_sender.send_email(
            subject=subject,
            recipients=recipients
        )
        
        if success:
            logger.info(f"âœ… æŠ“å–ç»“æœé‚®ä»¶å‘é€æˆåŠŸ: {', '.join(recipients)}")
            print(f"ğŸ“§ æŠ“å–ç»“æœé‚®ä»¶å·²å‘é€åˆ°: {', '.join(recipients)}")
        else:
            logger.error("âŒ æŠ“å–ç»“æœé‚®ä»¶å‘é€å¤±è´¥")
            print("âŒ æŠ“å–ç»“æœé‚®ä»¶å‘é€å¤±è´¥")
            
    except Exception as e:
        logger.error(f"âŒ å‘é€æŠ“å–ç»“æœé‚®ä»¶å¼‚å¸¸: {str(e)}")
        print(f"âŒ å‘é€æŠ“å–ç»“æœé‚®ä»¶å¤±è´¥: {str(e)}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸: {str(e)}")
        traceback.print_exc()
        sys.exit(1)